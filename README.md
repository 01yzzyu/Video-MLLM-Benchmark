# Video-MLLM-Benchmark

## Temporal Perception

| Date       | Keywords             | Institute (first)       | Paper                                                                                                                                                                                | Publication  | Code                                                                                                   | Project                                                                                 |
| :---------:| :------------------: | :---------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------: | :----------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------: |
| 2024-06    | MVBench              | OpenGVLab               | [MVBench: A Comprehensive Multi-modal Video Understanding Benchmark](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_MVBench_A_Comprehensive_Multi-modal_Video_Understanding_Benchmark_CVPR_2024_paper.pdf)        | CVPR 2024    | [Code](https://github.com/OpenGVLab/Ask-Anything)                                                     |                                                                                         |
| 2024-06    | TimeIT               | Ren Lab                 | [Timechat: A time-sensitive multimodal large language model for long video understanding](https://openaccess.thecvf.com/content/CVPR2024/papers/Ren_TimeChat_A_Time-sensitive_Multimodal_Large_Language_Model_for_Long_Video_CVPR_2024_paper.pdf) | CVPR 2024    | [Code](https://github.com/RenShuhuai-Andy/TimeChat)                                                   |                                                                                         |
| 2024-05    | ViLMA                | Cyberiada Lab           | [ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models](https://arxiv.org/pdf/2311.07022)                                                     | ICLR 2024    | [Code](https://cyberiada.github.io/ViLMA/)                                                            |                                                                                         |
| 2023-11    | VITATECS            | Peking University       | [VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models](https://arxiv.org/pdf/2311.17404)                                                      | arXiv 2023   | [Code](https://github.com/lscpku/VITATECS)                                                            |                                                                                         |
| 2024-03    | TempCompass          | TempCompass Lab         | [TempCompass: Do Video LLMs Really Understand Videos?](https://arxiv.org/pdf/2403.00476)                                                                                            | arXiv 2024   | [Code](https://github.com/llyx97/TempCompass)                                                        |                                                                                         |
| 2024-02    | OSCaR               | Nguyen Research         | [OSCaR: Object State Captioning and State Change Representation](https://arxiv.org/pdf/2402.17128)                                                                                 | arXiv 2024   | [Code](https://github.com/nguyennm1024/OSCaR)                                                        |                                                                                         |
| 2024-06    | ADLMCQ              | ADL-X Group             | [LLAVIDAL: Benchmarking Large Language Vision Models for Daily Activities of Living](https://arxiv.org/pdf/2406.09390)                                                              | arXiv 2024   | [Code](https://github.com/ADL-X/LLAVIDAL)                                                            |                                                                                         |
| 2024-12    | Perception Test     | DeepMind                | [Perception Test: A Diagnostic Benchmark for Multimodal Video Models](https://proceedings.neurips.cc/paper_files/paper/2023/file/8540fba4abdc7f9f7a7b1cc6cd60e409-Paper-Datasets_and_Benchmarks.pdf) | NeurIPS 2024 | [Code](https://github.com/google-deepmind/perception_test)                                           |                                                                                         |

## Long Video Understanding

| Date       | Keywords             | Institute (first)       | Paper                                                                                                                                                                                | Publication  | Code                                                                                                   | Project                                                                                 |
| :---------:| :------------------: | :---------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------: | :----------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------: |
| 2024-06    | MovieChat-1k         | Rese1f Lab              | [Moviechat: From dense token to sparse memory for long video understanding](https://openaccess.thecvf.com/content/CVPR2024/papers/Song_MovieChat_From_Dense_Token_to_Sparse_Memory_for_Long_Video_CVPR_2024_paper.pdf) | CVPR 2024    | [Code](https://github.com/rese1f/MovieChat)                                                           |                                                                                         |
| 2024-12    | EgoSchema            | EgoSchema Group         | [EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding](https://proceedings.neurips.cc/paper_files/paper/2023/file/90ce332aff156b910b002ce4e6880dec-Paper-Datasets_and_Benchmarks.pdf) | NeurIPS 2024 | [Code](https://egoschema.github.io/)                                                                 |                                                                                         |
| 2024-06    | Event-Bench          | RUCAIBox                | [Towards Event-oriented Long Video Understanding](https://arxiv.org/pdf/2406.14129)                                                                                                 | arXiv 2024   | [Code](https://github.com/RUCAIBox/Event-Bench)                                                      |                                                                                         |
| 2024-06    | MLVU                 | Junjie Lab              | [MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding](https://arxiv.org/abs/2406.04264)                                                                          | arXiv 2024   | [Code](https://github.com/JUNJIE99/MLVU)                                                              |                                                                                         |

## Comprehensive Evaluation

| Date       | Keywords             | Institute (first)       | Paper                                                                                                                                                                                | Publication  | Code                                                                                                   | Project                                                                                 |
| :---------:| :------------------: | :---------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------: | :----------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------: |
| 2023-11    | Video-Bench          | PKU-Yuan Group          | [Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models](https://arxiv.org/abs/2311.16103)                                             | arXiv 2023   | [Code](https://github.com/PKU-YuanGroup/Video-Bench)                                                 |                                                                                         |
| 2024-06    | MMBench-Video        | Open-Compass            | [MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding](https://arxiv.org/pdf/2406.14515)                                                                 | arXiv 2024   | [Code](https://github.com/open-compass/VLMEvalKit)                                                   |                                                                                         |
| 2024-06    | Video-MME            | Video-MME Lab           | [Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis](https://arxiv.org/pdf/2405.21075)                                               | arXiv 2024   | [Code](https://video-mme.github.io/)                                                                 |                                                                                         |
| 2023-11    | AutoEval-Video       | Xiuyuan Chen Lab        | [AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering](https://arxiv.org/pdf/2311.14906)                         | arXiv 2023   | [Code](https://github.com/Xiuyuan-Chen/AutoEval-Video)                                               |                                                                                         |
| 2024-06    | MMWorld              | MMWorld Group           | [MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos](https://arxiv.org/pdf/2406.08407)                                                                 | arXiv 2024   | [Code](https://mmworld-bench.github.io/)                                                             |                                                                                         |
| 2024-06    | WorldNet             | DCDM Lab                | [WorldGPT: Empowering LLM as Multimodal World Model](https://arxiv.org/pdf/2404.18202)                                                                                               | arXiv 2024   | [Code](https://github.com/DCDmllm/WorldGPT)                                                          |                                                                                         |
